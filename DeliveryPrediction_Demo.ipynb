{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T00:00:38.487543Z",
     "start_time": "2019-07-14T00:00:34.207265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T00:03:14.522139Z",
     "start_time": "2019-07-14T00:03:14.506530Z"
    }
   },
   "outputs": [],
   "source": [
    "import pgeocode\n",
    "from uszipcode import SearchEngine, SimpleZipcode, Zipcode\n",
    "\n",
    "def get_distance(zip1, zip2):\n",
    "    dist = pgeocode.GeoDistance('us')\n",
    "    return dist.query_postal_code(zip1, zip2)\n",
    "\n",
    "search = SearchEngine()\n",
    "def get_zip_details(zip):\n",
    "    zipcode = search.by_zipcode(zip)\n",
    "    pop = zipcode.population \n",
    "    pop_density = zipcode.population_density\n",
    "    housing_units = zipcode.housing_units\n",
    "    state = zipcode.state\n",
    "    # Return 0 if not found. nans will encounter error later.\n",
    "    return pop or 0, pop_density or 0, housing_units or 0, state or 0\n",
    "\n",
    "def add_missing_dummy_columns(d, columns ):\n",
    "    missing_cols = set( columns ) - set( d.columns )\n",
    "    for c in missing_cols:\n",
    "        d[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def preprocess(shipment_date, shipper, std_weight, freight_charges, zone, sender_zip, recipient_zip):\n",
    "    # Define standard service type\n",
    "    std_service_type = \"Ground\"\n",
    "    \n",
    "    # Get datetime features\n",
    "    \n",
    "    ## Convert string to pandas datetime series type\n",
    "    shipment_date_parsed = pd.Series(datetime.strptime(shipment_date, '%b %d %Y'))\n",
    "    \n",
    "    ## Get features from pandas series\n",
    "    week_number = shipment_date_parsed.dt.week.values[0]\n",
    "    day_of_week = shipment_date_parsed.dt.dayofweek.values[0]\n",
    "    month = shipment_date_parsed.dt.month.values[0]\n",
    "    \n",
    "    ## Get sender_in_MSA and recipient_in_MSA and same_MSA booleans\n",
    "    ##### COLTON\n",
    "    sender_in_MSA = 0 # to edit\n",
    "    rec_in_MSA = 0 # to edit\n",
    "    same_MSA = 0 # to edit\n",
    "    \n",
    "    ## Get distance\n",
    "    distance = get_distance(sender_zip, recipient_zip)\n",
    "    \n",
    "    ## Get population, density, no. houses, state code for recipient and sender\n",
    "    recipient_pop, recipient_pop_density, recipient_houses, recipient_state = get_zip_details(recipient_zip)\n",
    "    sender_pop, sender_pop_density, sender_houses, sender_state = get_zip_details(sender_zip)\n",
    "\n",
    "    # Populate dataframe\n",
    "    \n",
    "    ## Create empty dataframe with correct columns \n",
    "    feature_names = np.load('data/feature_names.npz')\n",
    "    df = pd.DataFrame(columns=feature_names['feature_names']) \n",
    "    \n",
    "    ## Add row into df\n",
    "    df.loc[0] = [shipper, std_service_type, std_weight, freight_charges, zone, \n",
    "                 sender_state, recipient_state, distance, sender_pop, sender_pop_density,\n",
    "                 sender_houses, recipient_pop, recipient_pop_density, recipient_houses, same_MSA,\n",
    "                 sender_in_MSA, rec_in_MSA, week_number, day_of_week, month]\n",
    "    \n",
    "    ## Define categorical and float columns\n",
    "    cat_cols = ['shipper','std_service_type','zone','week_number','day_of_week',\n",
    "                'sender_state','recipient_state','same_MSA', 'sender_in_MSA', 'rec_in_MSA',\n",
    "                'month']\n",
    "\n",
    "    float_cols = ['std_weight','freight_charges','distance', 'sender_pop', 'sender_pop_density',\n",
    "                 'sender_houses', 'recipient_pop', 'recipient_pop_density', 'recipient_houses']\n",
    "    \n",
    "    df[cat_cols] = df[cat_cols].astype('category')\n",
    "    df[float_cols] = df[float_cols].astype('float64')\n",
    "        \n",
    "    ## Dummify dataframe\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    ## Create empty dataframe in same shape as the one used in model, fill with 0s\n",
    "    df_full = pd.DataFrame(columns=feature_names['feature_names_dummified']) \n",
    "    \n",
    "    ## Execute a right join to align our test dataframe with full dataframe\n",
    "    df, df_full = df.align(df_full, join='right', axis=1, fill_value=0) \n",
    "    \n",
    "    ## Convert dataframe to numpy array for prediction\n",
    "#     X_test = df.loc[0]\n",
    "    X_test = df.loc[0].values\n",
    "    \n",
    "    ## Scale data with saved min-max scaler\n",
    "    ## scaler = joblib.load('model/scaler')\n",
    "    ## X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+01, 2.00000000e+01, 9.79671146e+00, 2.86150000e+04,\n",
       "       5.99100000e+03, 1.55850000e+04, 1.10810000e+04, 7.08300000e+03,\n",
       "       6.48900000e+03, 1.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = preprocess(\"Jul 25 2019\", \"fedex\", 10, 20, 3, 15206, 15211)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T00:03:14.506530Z",
     "start_time": "2019-07-14T00:02:47.902434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 1612557 rows, 30 columns\n",
      "Ending with 1612557 rows, 35 columns\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Features to be created:\n",
    "    1. sender_MSA_num\n",
    "    2. sender_in_MSA (binary 1 for MSA, 0 for MICRO)\n",
    "    3. recipient_MSA_num\n",
    "    4. recipient_in_MSA (binary 1 for MSA, 0 for MICRO) \n",
    "    \n",
    "Features will then be used to determine if the sender and recipient\n",
    "are in the same MSA.  These will be two additional\n",
    "features as shown below:\n",
    "    5. send_rec_same_MSA (binary)\n",
    "    \n",
    "Definition of CBSA, MSA, MICRO, and CSA as stated on inc.com:\n",
    "https://www.inc.com/encyclopedia/metropolitan-statistical-area-msa.html\n",
    "\n",
    "Core-Based Statistical Area\n",
    "A CBSA is one or more counties with an urbanized cluster of at least 10,000 people. \n",
    "The area as a whole is defined by the interaction between the core and the outlying areas. \n",
    "This interaction, measured by commuting, means that at least 25 percent of people in \n",
    "outlying areas are working in the core. The CBSA is a generic definition of MICROs and MSAs, \n",
    "the difference being core population size.\n",
    "\n",
    "Micropolitan Statistical Areas\n",
    "A MICRO is simply a small CBSA, i.e., a county or counties with an urbanized core of 10,000 \n",
    "but fewer than 50,000 in population. Outlying areas included are, again, defined by \n",
    "commuting patterns. As of November 2004, according to the Census Bureau, there were 575 MICROs \n",
    "in the U.S. and five in Puerto Rico.\n",
    "\n",
    "Metropolitan Statistical Areas\n",
    "An MSA has an urbanized core of minimally 50,000 population and includes outlying areas determined \n",
    "by commuting measures. In 2004, the U.S. had 362 MSAs and Puerto Rico eight.\n",
    "\n",
    "Combined Statistical Areas\n",
    "CSAs are two or more adjacent CBSAs in which there is at least a 15 employment interchange (measured \n",
    "by commuting) between cores. If this exchange is 25 percent or higher between a pair of CBSAs, they \n",
    "are combined into a CSA automatically; if the measure is at least 15 percent but below 25, local \n",
    "opinion in both areas is used to decide on combination. The U.S. had 116 CSAs in 2004.\n",
    "\n",
    "'''\n",
    "\n",
    "# Import zipcode to MSA map\n",
    "zipcode_to_MSA_df = pd.read_csv(cwd + \"\\\\data\\\\zip_to_MSA_numbers.csv\", dtype = object)\n",
    "\n",
    "cleaned_df_4 = cleaned_df_3.copy(deep=False)\n",
    "start_rows = len(cleaned_df_4)\n",
    "start_col = len(cleaned_df_4.columns)\n",
    "print(f\"Starting with {start_rows} rows, {start_col} columns\")\n",
    "\n",
    "# Change names of columns for simplicity in coding\n",
    "zipcode_to_MSA_df.columns = ['zipcode', 'state', 'msa_num', 'county_num', 'msa_name']\n",
    "\n",
    "# Creating dictionary for mapping zipcodes to MSA numbers\n",
    "zip_msa_num_dict = zipcode_to_MSA_df.set_index('zipcode')['msa_num'].to_dict()\n",
    "zip_msa_name_dict = zipcode_to_MSA_df.set_index('zipcode')['msa_name'].to_dict()\n",
    "\n",
    "# Lists to be filled and then converted to dataframe column features\n",
    "sender_MSA_num = []\n",
    "sender_in_MSA = []\n",
    "recipient_MSA_num = []\n",
    "recipient_in_MSA = []\n",
    "send_rec_same_MSA = []\n",
    "\n",
    "# For debugging purposes (find zipcodes that don't show up in dictionary)\n",
    "zips_not_in_dict = {}\n",
    "\n",
    "for row in cleaned_df_4.itertuples():\n",
    "    if row.sender_zip in zip_msa_num_dict:\n",
    "        sender_MSA_num.append(zip_msa_num_dict[row.sender_zip])\n",
    "        msa_name = zip_msa_name_dict[row.sender_zip]\n",
    "        if 'MSA' in msa_name:\n",
    "            sender_in_MSA.append(1)\n",
    "        else:\n",
    "            sender_in_MSA.append(0)\n",
    "    else:\n",
    "        sender_in_MSA.append(0)\n",
    "        sender_MSA_num.append(0)\n",
    "        if row.sender_zip not in zips_not_in_dict:\n",
    "            zips_not_in_dict[row.sender_zip] = 1\n",
    "        else:\n",
    "            zips_not_in_dict[row.sender_zip] += 1\n",
    "    if row.recipient_zip in zip_msa_num_dict:\n",
    "        recipient_MSA_num.append(zip_msa_num_dict[row.recipient_zip])\n",
    "        msa_name = zip_msa_name_dict[row.recipient_zip]\n",
    "        if 'MSA' in msa_name:\n",
    "            recipient_in_MSA.append(1)\n",
    "        else:\n",
    "            recipient_in_MSA.append(0)\n",
    "    else:\n",
    "        recipient_MSA_num.append(0)\n",
    "        recipient_in_MSA.append(0)\n",
    "        if row.recipient_zip not in zips_not_in_dict:\n",
    "            zips_not_in_dict[row.recipient_zip] = 1\n",
    "        else:\n",
    "            zips_not_in_dict[row.recipient_zip] += 1\n",
    "        \n",
    "# Checking to see if sender and recipient are in same MSA and filling list\n",
    "for s, r in zip(sender_MSA_num, recipient_MSA_num):\n",
    "    if s == r:\n",
    "        send_rec_same_MSA.append(1)\n",
    "    else:\n",
    "        send_rec_same_MSA.append(0)\n",
    "\n",
    "# Creating columns and adding to dataframe\n",
    "cleaned_df_4['same_MSA'] = pd.Series(send_rec_same_MSA)\n",
    "cleaned_df_4['sender_in_MSA'] = pd.Series(sender_in_MSA)\n",
    "cleaned_df_4['rec_in_MSA'] = pd.Series(recipient_in_MSA)\n",
    "cleaned_df_4['sender_MSA_num'] = pd.Series(sender_MSA_num)\n",
    "cleaned_df_4['rec_MSA_num'] = pd.Series(recipient_MSA_num)\n",
    "\n",
    "end_rows = len(cleaned_df_4)\n",
    "end_col = len(cleaned_df_4.columns)\n",
    "print(f\"Ending with {end_rows} rows, {end_col} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
