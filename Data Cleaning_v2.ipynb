{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the following script, install the following via a terminal:\n",
    "\n",
    "- pip install uszipcode\n",
    "- pip install --upgrade uszipcode (if needed)\n",
    "- pip install python-Levenshtein\n",
    "- pip install fuzzywuzzy\n",
    "\n",
    "Levenshtein may require a Microsoft C++ build package through Visual Studio. Follow prompts/errors as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import MySQLdb as db\n",
    "import credentials\n",
    "import pickle\n",
    "import time\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish connection to the MySQLDB\n",
    "MySQLDB = db.connect(credentials.host, credentials.user, credentials.password, credentials.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized shipping methods based primarily upon what is selectable through the FedEx API here:\n",
    "# https://www.fedex.com/ratefinder/home. 'Home Delivery' and 'Smartpost' are not selectable.\n",
    "fedex_methods = ['Same Day', 'First Overnight', 'Priority Overnight', 'First Overnight',\n",
    "                 'Priority Overnight', 'Standard Overnight', '2Day AM', '2Day', 'Express Saver',\n",
    "                 'Ground','Home Delivery','Smartpost']\n",
    "\n",
    "# Standardized shipping methods based primarily on what is selectable through the API here:\n",
    "# https://wwwapps.ups.com/ctc/request?loc=en_US. 'Surepost' and 'Standard' are not selectable.\n",
    "ups_methods = ['Next Day Air Early', 'Next Day Air', 'Next Day Air Saver', '2nd Day Air A.M.',\n",
    "               '2nd Day Air', '3 Day Select', 'Ground', 'Surepost', 'Standard']\n",
    "\n",
    "# Standardized state names and codes of the 48 contiguous states based upon USPS standards found here:\n",
    "# https://www.ups.com/worldshiphelp/WS14/ENU/AppHelp/Codes/State_Province_Codes.htm\n",
    "state_codes = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA', 'ID', 'IL', 'IN', \n",
    "               'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH',\n",
    "               'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "               'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "# Instantiates variable for searching for state from zip\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "# Instantiates files for reading from and writing to a pickle file\n",
    "file_path = \"data/data.pickle\"\n",
    "pkl_file = open(file_path,\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(lower,upper):\n",
    "    \"\"\"\n",
    "    Queries for and returns a sample of records that meet where clause criteria\n",
    "    \"\"\"\n",
    "    # initializes query based upon lower and upper data inputs\n",
    "    sql_query = \"\"\"\n",
    "    select * from \n",
    "    (select * from \n",
    "    (select * from \n",
    "    (select year_week, business_sid, upper(trim(industry)) as industry, upper(trim(sub_industry)) as sub_industry,shipper,\n",
    "    trim(service_type_description) as service_type,package_count, weight,shipment_date,delivery_date, delivery_time, \n",
    "    freight_charges,freight_discount_amount,misc_charges,misc_discount_amount, \n",
    "    net_charge_amount, zone, upper(trim(sender_city)) as sender_city, upper(trim(sender_state)) as sender_state,\n",
    "    left(sender_zip,5) as sender_zip, upper(trim(recipient_city)) as recipient_city,\n",
    "    upper(trim(recipient_state)) as recipient_state, left(recipient_zip,5) as recipient_zip \n",
    "    from libras.shipment_details \n",
    "    where sender_country = 'US' and recipient_country = 'US' and year_week >= {} and year_week < {}\n",
    "    and delivery_date is not null\n",
    "    ) t1 \n",
    "    where t1.shipment_date is not null) t2 \n",
    "    where t2.freight_charges > 0) t3 \n",
    "    where t3.zone is not null or trim(zone)!='' \n",
    "    \"\"\".format(lower,upper)\n",
    "    \n",
    "    # queries database and samples results\n",
    "    records = pd.read_sql_query(sql_query, MySQLDB).sample(frac = 0.14, replace = False) \n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(records):\n",
    "    \"\"\"\n",
    "    Preprocesses records to satisfy common cleansing requirements between benchmarking and delivery prediction solutions\n",
    "    \"\"\"    \n",
    "    # sets float dtypes and standardizes zones to single digits    \n",
    "    float_cols = ['freight_charges','freight_discount_amount','misc_charges','misc_discount_amount','net_charge_cmount','zone']\n",
    "    records = records[records.zone.apply(lambda x: x.isnumeric())]\n",
    "    records[float_cols] = records[float_cols].astype('float64')\n",
    "    records.zone %= 10\n",
    "\n",
    "    # strips leading and trailing whitespaces from all string values\n",
    "    obj_columns = records.select_dtypes(include='object').columns + ['zone']\n",
    "    for column in obj_columns:\n",
    "        records[column] = records[column].str.strip()\n",
    "    \n",
    "    # converts a subset columns of dtype 'object' to dtype 'category' for memory conservation and later ml use \n",
    "    cat_columns = ['industry','sub_industry','sender_state','recipient_state','zone']\n",
    "    records[cat_cols] = records[cat_cols].astype('category')\n",
    "    \n",
    "    # creates std_weight (weight/package_count)\n",
    "    records.insert(8, 'std_weight', records['weight'] / records['package_count'])\n",
    "    \n",
    "    # applies fuzzy macthing to each service type relative to the standardized list per carrier.\n",
    "    service_type_fuzzy_match = []\n",
    "    columns = ['shipper','service_type']\n",
    "    for record in records[columns].itertuples():       \n",
    "        if record.shipper == 'fedex':\n",
    "            service_type_fuzzy_match.append(process.extractOne(record.service_type, fedex_methods, scorer = fuzz.partial_ratio))\n",
    "        else:\n",
    "            service_type_fuzzy_match.append(process.extractOne(record.service_type, ups_methods, scorer = fuzz.partial_ratio))  \n",
    "            \n",
    "    # adds the standardized service type and drops all records with service type scores less than 70\n",
    "    records.insert(6, 'std_service_type', [method for method, score in service_type_fuzzy_match])\n",
    "    records = records.assign(std_service_type_score = [score for method, score in service_type_fuzzy_match])\n",
    "    records = records[records.std_service_type_score >= 70]\n",
    "    \n",
    "    # removes records with sender or recipient states residing outside of the 48 contiguous states\n",
    "    records = records[(records.recipient_state.isin(state_codes + ['']))]\n",
    "    records = records[(records.sender_state.isin(state_codes + ['']))]\n",
    "    \n",
    "    # drops unneeded columns \n",
    "    records = records.drop(['std_service_type_score'], axis=1)\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(records):\n",
    "    \"\"\"\n",
    "    Stores results from each query into a pickle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_df = pd.DataFrame()\n",
    "        new_df = pd.read_pickle(file_path)\n",
    "        print(\"Merged pickle contains\", len(new_df.index), \"records before merge\")\n",
    "        temp_df = new_df.append(records, sort=False)\n",
    "        temp_df.to_pickle(file_path)\n",
    "        print(\"Merged pickle contains\", len(temp_df.index), \"records after merge\")\n",
    "    except EOFError:\n",
    "        records.to_pickle(file_path)\n",
    "        print(\"Merged pickle contains\", len(records.index), \"records with initial merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(start_time_, batch_num_, lower_date_range_, upper_date_range_):\n",
    "    batch_start_time = time.time()\n",
    "    print(\"Initiating batch ({}): year_week {} and {} (exclusive)\".format(\n",
    "        batch_num_, lower_date_range_, upper_date_range_))\n",
    "    \n",
    "    # queries for a batch of records within a given 4-week range\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Querying records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    queried_results = query(lower = lower_date_range_, upper = upper_date_range_)\n",
    "    \n",
    "    # preprocesses the record batch\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Preprocessing records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    preprocessed_results = preprocess(queried_results)\n",
    "    \n",
    "    # appends the record batch\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Storing records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    store(preprocessed_results)\n",
    "    \n",
    "    # reports final time\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Records stored\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating batch (1): year_week 201823 and 201827 (exclusive)\n",
      "Total time: 0 min | Batch time: 0 sec | Querying records\n",
      "Total time: 1 min | Batch time: 104 sec | Preprocessing records\n",
      "Total time: 2 min | Batch time: 154 sec | Storing records\n",
      "Merged pickle contains 226803 records with initial merge\n",
      "Total time: 2 min | Batch time: 155 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (2): year_week 201827 and 201831 (exclusive)\n",
      "Total time: 2 min | Batch time: 0 sec | Querying records\n",
      "Total time: 4 min | Batch time: 97 sec | Preprocessing records\n",
      "Total time: 5 min | Batch time: 143 sec | Storing records\n",
      "Merged pickle contains 226803 records before merge\n",
      "Merged pickle contains 431513 records after merge\n",
      "Total time: 5 min | Batch time: 146 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (3): year_week 201831 and 201835 (exclusive)\n",
      "Total time: 5 min | Batch time: 0 sec | Querying records\n",
      "Total time: 6 min | Batch time: 111 sec | Preprocessing records\n",
      "Total time: 7 min | Batch time: 172 sec | Storing records\n",
      "Merged pickle contains 431513 records before merge\n",
      "Merged pickle contains 673350 records after merge\n",
      "Total time: 8 min | Batch time: 178 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (4): year_week 201835 and 201839 (exclusive)\n",
      "Total time: 8 min | Batch time: 0 sec | Querying records\n",
      "Total time: 10 min | Batch time: 148 sec | Preprocessing records\n",
      "Total time: 11 min | Batch time: 211 sec | Storing records\n",
      "Merged pickle contains 673350 records before merge\n",
      "Merged pickle contains 934539 records after merge\n",
      "Total time: 11 min | Batch time: 219 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (5): year_week 201839 and 201843 (exclusive)\n",
      "Total time: 11 min | Batch time: 0 sec | Querying records\n",
      "Total time: 14 min | Batch time: 149 sec | Preprocessing records\n",
      "Total time: 15 min | Batch time: 229 sec | Storing records\n",
      "Merged pickle contains 934539 records before merge\n",
      "Merged pickle contains 1214793 records after merge\n",
      "Total time: 15 min | Batch time: 240 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (6): year_week 201843 and 201847 (exclusive)\n",
      "Total time: 15 min | Batch time: 0 sec | Querying records\n",
      "Total time: 18 min | Batch time: 130 sec | Preprocessing records\n",
      "Total time: 19 min | Batch time: 187 sec | Storing records\n",
      "Merged pickle contains 1214793 records before merge\n",
      "Merged pickle contains 1468067 records after merge\n",
      "Total time: 19 min | Batch time: 200 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (7): year_week 201847 and 201851 (exclusive)\n",
      "Total time: 19 min | Batch time: 0 sec | Querying records\n",
      "Total time: 21 min | Batch time: 153 sec | Preprocessing records\n",
      "Total time: 22 min | Batch time: 222 sec | Storing records\n",
      "Merged pickle contains 1468067 records before merge\n",
      "Merged pickle contains 1767227 records after merge\n",
      "Total time: 23 min | Batch time: 236 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (8): year_week 201851 and 201855 (exclusive)\n",
      "Total time: 23 min | Batch time: 0 sec | Querying records\n",
      "Total time: 24 min | Batch time: 95 sec | Preprocessing records\n",
      "Total time: 25 min | Batch time: 140 sec | Storing records\n",
      "Merged pickle contains 1767227 records before merge\n",
      "Merged pickle contains 1959549 records after merge\n",
      "Total time: 25 min | Batch time: 157 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (9): year_week 201901 and 201905 (exclusive)\n",
      "Total time: 25 min | Batch time: 0 sec | Querying records\n",
      "Total time: 28 min | Batch time: 125 sec | Preprocessing records\n",
      "Total time: 28 min | Batch time: 184 sec | Storing records\n",
      "Merged pickle contains 1959549 records before merge\n",
      "Merged pickle contains 2196996 records after merge\n",
      "Total time: 29 min | Batch time: 205 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (10): year_week 201905 and 201909 (exclusive)\n",
      "Total time: 29 min | Batch time: 0 sec | Querying records\n",
      "Total time: 31 min | Batch time: 138 sec | Preprocessing records\n",
      "Total time: 32 min | Batch time: 197 sec | Storing records\n",
      "Merged pickle contains 2196996 records before merge\n",
      "Merged pickle contains 2446903 records after merge\n",
      "Total time: 33 min | Batch time: 218 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (11): year_week 201909 and 201913 (exclusive)\n",
      "Total time: 33 min | Batch time: 0 sec | Querying records\n",
      "Total time: 35 min | Batch time: 150 sec | Preprocessing records\n",
      "Total time: 36 min | Batch time: 215 sec | Storing records\n",
      "Merged pickle contains 2446903 records before merge\n",
      "Merged pickle contains 2726820 records after merge\n",
      "Total time: 37 min | Batch time: 240 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (12): year_week 201913 and 201917 (exclusive)\n",
      "Total time: 37 min | Batch time: 0 sec | Querying records\n",
      "Total time: 39 min | Batch time: 144 sec | Preprocessing records\n",
      "Total time: 40 min | Batch time: 214 sec | Storing records\n",
      "Merged pickle contains 2726820 records before merge\n",
      "Merged pickle contains 3005306 records after merge\n",
      "Total time: 41 min | Batch time: 242 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (13): year_week 201917 and 201921 (exclusive)\n",
      "Total time: 41 min | Batch time: 0 sec | Querying records\n",
      "Total time: 43 min | Batch time: 164 sec | Preprocessing records\n",
      "Total time: 45 min | Batch time: 234 sec | Storing records\n",
      "Merged pickle contains 3005306 records before merge\n",
      "Merged pickle contains 3292978 records after merge\n",
      "Total time: 45 min | Batch time: 265 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (14): year_week 201921 and 201925 (exclusive)\n",
      "Total time: 45 min | Batch time: 0 sec | Querying records\n",
      "Total time: 48 min | Batch time: 161 sec | Preprocessing records\n",
      "Total time: 49 min | Batch time: 230 sec | Storing records\n",
      "Merged pickle contains 3292978 records before merge\n",
      "Merged pickle contains 3580760 records after merge\n",
      "Total time: 50 min | Batch time: 262 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (15): year_week 201925 and 201929 (exclusive)\n",
      "Total time: 50 min | Batch time: 0 sec | Querying records\n",
      "Total time: 50 min | Batch time: 29 sec | Preprocessing records\n",
      "Total time: 50 min | Batch time: 45 sec | Storing records\n",
      "Merged pickle contains 3580760 records before merge\n",
      "Merged pickle contains 3651428 records after merge\n",
      "Total time: 51 min | Batch time: 76 sec | Records stored\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "batch_num = 1\n",
    "\n",
    "#Get records for 2018\n",
    "min_date = 201823\n",
    "max_date = 201852 #201852\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201852:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        process_batch(start_time, batch_num, lower_date_range, upper_date_range)\n",
    "        lower_date_range=upper_date_range\n",
    "        batch_num += 1\n",
    "\n",
    "#Get records for 2019        \n",
    "min_date = 201901\n",
    "max_date = 201925 #201925\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201925:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        process_batch(start_time, batch_num, lower_date_range, upper_date_range)\n",
    "        lower_date_range=upper_date_range\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file.close()\n",
    "pd.read_pickle(file_path).sample(frac=0.25, replace=False).to_pickle('data/data_sample.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
