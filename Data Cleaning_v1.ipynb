{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector as db\n",
    "import credentials\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Establish connection to the MySQLDB\n",
    "MySQLDB = db.connect(host = credentials.host, user = credentials.user, password = credentials.password,\n",
    "                     db = credentials.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two pickle files are used for this activity\n",
    "#The test_indiv pickle contains intermediate records for each month's query\n",
    "#The test_merged pickle contains all records across every month\n",
    "pkl_indiv = open(\"test_indiv.pickle\",\"wb\")\n",
    "pkl_merged = open(\"test_merged.pickle\",\"wb\")\n",
    "\n",
    "def execute_query(lower,upper):\n",
    "    \"\"\"\n",
    "    Execute a query for a given date range\n",
    "    \"\"\"\n",
    "    sql_query = \"\"\"\n",
    "    select * from \n",
    "    (select * from \n",
    "    (select * from \n",
    "    (select year_week, business_sid, upper(trim(industry)) as industry, upper(trim(sub_industry)) as sub_industry,shipper,\n",
    "    trim(service_type_description) as service_type,package_count, weight,shipment_date,delivery_date, delivery_time, \n",
    "    freight_charges,freight_discount_amount,misc_charges,misc_discount_amount, \n",
    "    net_charge_amount, zone, upper(trim(sender_city)) as sender_city, left(sender_zip,5) as sender_zip,\n",
    "    upper(trim(recipient_city)) as recipient_city,left(recipient_zip,5) as recipient_zip \n",
    "    from libras.shipment_details \n",
    "    where sender_country = 'US' and recipient_country = 'US' and year_week >= {} and year_week < {}\n",
    "    and delivery_date is not null \n",
    "    ) t1 \n",
    "    where t1.shipment_date is not null) t2 \n",
    "    where t2.freight_charges > 0) t3 \n",
    "    where t3.zone is not null or trim(zone)!='' \n",
    "    limit 300000\"\"\".format(lower,upper)\n",
    "    #print(sql_query)\n",
    "    cursor = MySQLDB.cursor()\n",
    "    cursor.execute(sql_query)\n",
    "    records = pd.DataFrame(cursor.fetchall())\n",
    "    records.to_pickle(\"test_indiv.pickle\")\n",
    "    return records \n",
    "    #print(len(records.index))\n",
    "\n",
    "def store_results(monthly_records_df):\n",
    "    \"\"\"\n",
    "    Store results from each query into a pickle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_df = pd.DataFrame()\n",
    "        newDF = pd.read_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(newDF.index), \"records before merge\")\n",
    "        temp_df = newDF.append(monthly_records_df)\n",
    "        temp_df.to_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(temp_df.index), \"records after merge\")\n",
    "    except EOFError:\n",
    "        monthly_records_df.to_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(monthly_records_df.index), \"records with initial merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records between year_week  201823  and  201827  (exclusive)\n",
      "Merged pickle contains  300000  records with initial merge\n",
      "Getting records between year_week  201827  and  201831  (exclusive)\n",
      "Merged pickle contains  300000  records before merge\n",
      "Merged pickle contains  600000  records after merge\n",
      "Getting records between year_week  201831  and  201835  (exclusive)\n",
      "Merged pickle contains  600000  records before merge\n",
      "Merged pickle contains  900000  records after merge\n",
      "Getting records between year_week  201835  and  201839  (exclusive)\n",
      "Merged pickle contains  900000  records before merge\n",
      "Merged pickle contains  1200000  records after merge\n",
      "Getting records between year_week  201839  and  201843  (exclusive)\n",
      "Merged pickle contains  1200000  records before merge\n",
      "Merged pickle contains  1500000  records after merge\n",
      "Getting records between year_week  201843  and  201847  (exclusive)\n",
      "Merged pickle contains  1500000  records before merge\n",
      "Merged pickle contains  1800000  records after merge\n",
      "Getting records between year_week  201847  and  201851  (exclusive)\n",
      "Merged pickle contains  1800000  records before merge\n",
      "Merged pickle contains  2100000  records after merge\n",
      "Getting records between year_week  201851  and  201855  (exclusive)\n",
      "Merged pickle contains  2100000  records before merge\n",
      "Merged pickle contains  2400000  records after merge\n",
      "Getting records between year_week  201901  and  201905  (exclusive)\n",
      "Merged pickle contains  2400000  records before merge\n",
      "Merged pickle contains  2700000  records after merge\n",
      "Getting records between year_week  201905  and  201909  (exclusive)\n",
      "Merged pickle contains  2700000  records before merge\n",
      "Merged pickle contains  3000000  records after merge\n",
      "Getting records between year_week  201909  and  201913  (exclusive)\n",
      "Merged pickle contains  3000000  records before merge\n",
      "Merged pickle contains  3300000  records after merge\n",
      "Getting records between year_week  201913  and  201917  (exclusive)\n",
      "Merged pickle contains  3300000  records before merge\n"
     ]
    }
   ],
   "source": [
    "#Get records for 2018\n",
    "min_date = 201823\n",
    "max_date = 201852 #201852\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201852:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        print(\"Getting records between year_week\", lower_date_range,\"and\",upper_date_range, \"(exclusive)\") \n",
    "        store_results(execute_query(lower = lower_date_range,upper = upper_date_range))\n",
    "        lower_date_range=upper_date_range\n",
    "\n",
    "#Get records for 2019        \n",
    "min_date = 201901\n",
    "max_date = 201925 #201925\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201925:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        print(\"Getting records between year_week\", lower_date_range,\"and\",upper_date_range, \"(exclusive)\") \n",
    "        store_results(execute_query(lower = lower_date_range,upper = upper_date_range))\n",
    "        lower_date_range=upper_date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(\"test_merged.pickle\")\n",
    "len(unpickled_df)\n",
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unpickled_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-01035e67d14a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Code to replace zones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munpickled_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munpickled_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnumeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0munpickled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zone'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zone'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpickled_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0munpickled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zone'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickled_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zone'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unpickled_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Code to replace zones \n",
    "unpickled_df = unpickled_df[unpickled_df.zone.apply(lambda x: x.isnumeric())]\n",
    "unpickled_df['zone'] = unpickled_df['zone'].unpickled_df(int)\n",
    "unpickled_df['zone'] = unpickled_df['zone'] % 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
