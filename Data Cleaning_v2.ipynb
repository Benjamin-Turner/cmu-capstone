{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import MySQLdb as db\n",
    "import credentials\n",
    "import pickle\n",
    "import time\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 82.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Establish connection to the MySQLDB\n",
    "MySQLDB = db.connect(credentials.host, credentials.user, credentials.password, credentials.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized shipping methods based primarily upon what is selectable through the FedEx API here:\n",
    "# https://www.fedex.com/ratefinder/home. 'Home Delivery' and 'Smartpost' are not selectable.\n",
    "fedex_methods = ['Same Day', 'First Overnight', 'Priority Overnight', 'First Overnight',\n",
    "                 'Priority Overnight', 'Standard Overnight', '2Day AM', '2Day', 'Express Saver',\n",
    "                 'Ground','Home Delivery','Smartpost']\n",
    "\n",
    "# Standardized shipping methods based primarily on what is selectable through the API here:\n",
    "# https://wwwapps.ups.com/ctc/request?loc=en_US. 'Surepost' and 'Standard' are not selectable.\n",
    "ups_methods = ['Next Day Air Early', 'Next Day Air', 'Next Day Air Saver', '2nd Day Air A.M.',\n",
    "               '2nd Day Air', '3 Day Select', 'Ground', 'Surepost', 'Standard']\n",
    "\n",
    "# Standardized state names and codes of the 48 contiguous states based upon USPS standards found here:\n",
    "# https://www.ups.com/worldshiphelp/WS14/ENU/AppHelp/Codes/State_Province_Codes.htm\n",
    "state_codes = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', \n",
    "               'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH',\n",
    "               'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "               'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "pkl_merged = open(\"test_merged.pickle\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(lower,upper):\n",
    "    \"\"\"\n",
    "    Queries for and returns a sample of records that meet where clause criteria\n",
    "    \"\"\"\n",
    "    # initializes query based upon lower and upper data inputs\n",
    "    sql_query = \"\"\"\n",
    "    select * from \n",
    "    (select * from \n",
    "    (select * from \n",
    "    (select year_week, business_sid, upper(trim(industry)) as industry, upper(trim(sub_industry)) as sub_industry,shipper,\n",
    "    trim(service_type_description) as service_type,package_count, weight,shipment_date,delivery_date, delivery_time, \n",
    "    freight_charges,freight_discount_amount,misc_charges,misc_discount_amount, \n",
    "    net_charge_amount, zone, upper(trim(sender_city)) as sender_city, upper(trim(sender_state)) as sender_state,\n",
    "    left(sender_zip,5) as sender_zip, upper(trim(recipient_city)) as recipient_city,\n",
    "    upper(trim(recipient_state)) as recipient_state, left(recipient_zip,5) as recipient_zip \n",
    "    from libras.shipment_details \n",
    "    where sender_country = 'US' and recipient_country = 'US' and year_week >= {} and year_week < {}\n",
    "    and delivery_date is not null\n",
    "    ) t1 \n",
    "    where t1.shipment_date is not null) t2 \n",
    "    where t2.freight_charges > 0) t3 \n",
    "    where t3.zone is not null or trim(zone)!='' \n",
    "    \"\"\".format(lower,upper)\n",
    "    \n",
    "    # queries database and samples results\n",
    "    records = pd.read_sql_query(sql_query, MySQLDB).sample(frac = 0.14, replace = False) \n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(records):\n",
    "    \"\"\"\n",
    "    Preprocesses records to satisfy common cleansing requirements between benchmarking and delivery prediction solutions\n",
    "    \"\"\"\n",
    "    # sets datatypes and standardizes zones to single digits\n",
    "    records = records[records.zone.apply(lambda x: x.isnumeric())]\n",
    "    records = records.astype({'freight_charges':'float64',\n",
    "                              'freight_discount_amount':'float64',\n",
    "                              'misc_charges':'float64',\n",
    "                              'misc_discount_amount':'float64',\n",
    "                              'net_charge_amount':'float64',\n",
    "                              'zone':'int64'})\n",
    "    records.zone %= 10\n",
    "    records = records.astype({'zone':'str'})\n",
    "    \n",
    "    # Applies the 'fuzz.partial_ratio' fuzzy macthing algorithm to each record based upon the record's service_type.\n",
    "    # The partial_ratio function is designed to return the shipping method with the highest score as a two pair tuple\n",
    "    records_fuzzy_match = []\n",
    "    for row in records[['shipper','service_type']].itertuples():\n",
    "        if row.shipper == 'fedex':\n",
    "            records_fuzzy_match.append(process.extractOne(\n",
    "                row.service_type,\n",
    "                fedex_methods,\n",
    "                scorer = fuzz.partial_ratio))\n",
    "        else:\n",
    "            records_fuzzy_match.append(process.extractOne(\n",
    "                row.service_type,\n",
    "                ups_methods,\n",
    "                scorer = fuzz.partial_ratio))\n",
    "     \n",
    "    # adds the two-element tuple results of the fuzzy match as additional columns and filters out all records\n",
    "    # with a score less than 70. Drops the score column after filter is applied.\n",
    "    records.insert(6, 'std_service_type', [method for method, score in records_fuzzy_match])\n",
    "    records = records.assign(std_service_type_score = [score for method, score in records_fuzzy_match])\n",
    "    records = records[records.std_service_type_score >= 70]\n",
    "    records = records.drop('std_service_type_score', axis=1)\n",
    "    \n",
    "    # removes records with sender or recipient states residing outside of the 48 contiguous states\n",
    "    records[(~records.recipient_state.isin(state_codes)) & (records.recipient_state != '')]\n",
    "    records[(~records.sender_state.isin(state_codes)) & (records.sender_state != '')]\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(monthly_records_df):\n",
    "    \"\"\"\n",
    "    Stores results from each query into a pickle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temp_df = pd.DataFrame()\n",
    "        newDF = pd.read_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(newDF.index), \"records before merge\")\n",
    "        temp_df = newDF.append(monthly_records_df)\n",
    "        temp_df.to_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(temp_df.index), \"records after merge\")\n",
    "    except EOFError:\n",
    "        monthly_records_df.to_pickle(\"test_merged.pickle\")\n",
    "        print(\"Merged pickle contains\", len(monthly_records_df.index), \"records with initial merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(start_time_, batch_num_, lower_date_range_, upper_date_range_):\n",
    "    batch_start_time = time.time()\n",
    "    print(\"Initiating batch ({}): year_week {} and {} (exclusive)\".format(\n",
    "        batch_num_, lower_date_range_, upper_date_range_))\n",
    "    \n",
    "    # queries for a batch of records within a given 4-week range\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Querying records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    queried_results = query(lower = lower_date_range_, upper = upper_date_range_)\n",
    "    \n",
    "    # preprocesses the record batch\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Preprocessing records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    preprocessed_results = preprocess(queried_results)\n",
    "    \n",
    "    # appends the record batch\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Storing records\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    store(preprocessed_results)\n",
    "    \n",
    "    # reports final time\n",
    "    print(\"Total time: {} min | Batch time: {} sec | Records stored\".format(\n",
    "        int(round((time.time() - start_time_)/60, 2)),\n",
    "        int(round(time.time() - batch_start_time))))\n",
    "    print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating batch (1): year_week 201823 and 201827 (exclusive)\n",
      "Total time: 0 min | Batch time: 0 sec | Querying records\n",
      "Total time: 1 min | Batch time: 69 sec | Preprocessing records\n",
      "Total time: 1 min | Batch time: 107 sec | Storing records\n",
      "Merged pickle contains 227240 records with initial merge\n",
      "Total time: 1 min | Batch time: 109 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (2): year_week 201827 and 201831 (exclusive)\n",
      "Total time: 1 min | Batch time: 0 sec | Querying records\n",
      "Total time: 3 min | Batch time: 75 sec | Preprocessing records\n",
      "Total time: 3 min | Batch time: 110 sec | Storing records\n",
      "Merged pickle contains 227240 records before merge\n",
      "Merged pickle contains 432291 records after merge\n",
      "Total time: 3 min | Batch time: 113 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (3): year_week 201831 and 201835 (exclusive)\n",
      "Total time: 3 min | Batch time: 0 sec | Querying records\n",
      "Total time: 5 min | Batch time: 90 sec | Preprocessing records\n",
      "Total time: 5 min | Batch time: 129 sec | Storing records\n",
      "Merged pickle contains 432291 records before merge\n",
      "Merged pickle contains 674542 records after merge\n",
      "Total time: 5 min | Batch time: 134 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (4): year_week 201835 and 201839 (exclusive)\n",
      "Total time: 6 min | Batch time: 0 sec | Querying records\n",
      "Total time: 7 min | Batch time: 101 sec | Preprocessing records\n",
      "Total time: 8 min | Batch time: 156 sec | Storing records\n",
      "Merged pickle contains 674542 records before merge\n",
      "Merged pickle contains 936241 records after merge\n",
      "Total time: 8 min | Batch time: 163 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (5): year_week 201839 and 201843 (exclusive)\n",
      "Total time: 8 min | Batch time: 0 sec | Querying records\n",
      "Total time: 10 min | Batch time: 112 sec | Preprocessing records\n",
      "Total time: 11 min | Batch time: 161 sec | Storing records\n",
      "Merged pickle contains 936241 records before merge\n",
      "Merged pickle contains 1216913 records after merge\n",
      "Total time: 11 min | Batch time: 170 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (6): year_week 201843 and 201847 (exclusive)\n",
      "Total time: 11 min | Batch time: 0 sec | Querying records\n",
      "Total time: 13 min | Batch time: 95 sec | Preprocessing records\n",
      "Total time: 14 min | Batch time: 143 sec | Storing records\n",
      "Merged pickle contains 1216913 records before merge\n",
      "Merged pickle contains 1470689 records after merge\n",
      "Total time: 14 min | Batch time: 154 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (7): year_week 201847 and 201851 (exclusive)\n",
      "Total time: 14 min | Batch time: 0 sec | Querying records\n",
      "Total time: 16 min | Batch time: 114 sec | Preprocessing records\n",
      "Total time: 17 min | Batch time: 171 sec | Storing records\n",
      "Merged pickle contains 1470689 records before merge\n",
      "Merged pickle contains 1770248 records after merge\n",
      "Total time: 17 min | Batch time: 183 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (8): year_week 201851 and 201855 (exclusive)\n",
      "Total time: 17 min | Batch time: 0 sec | Querying records\n",
      "Total time: 18 min | Batch time: 78 sec | Preprocessing records\n",
      "Total time: 19 min | Batch time: 108 sec | Storing records\n",
      "Merged pickle contains 1770248 records before merge\n",
      "Merged pickle contains 1963004 records after merge\n",
      "Total time: 19 min | Batch time: 121 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (9): year_week 201901 and 201905 (exclusive)\n",
      "Total time: 19 min | Batch time: 0 sec | Querying records\n",
      "Total time: 21 min | Batch time: 97 sec | Preprocessing records\n",
      "Total time: 21 min | Batch time: 138 sec | Storing records\n",
      "Merged pickle contains 1963004 records before merge\n",
      "Merged pickle contains 2200989 records after merge\n",
      "Total time: 22 min | Batch time: 154 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (10): year_week 201905 and 201909 (exclusive)\n",
      "Total time: 22 min | Batch time: 0 sec | Querying records\n",
      "Total time: 23 min | Batch time: 99 sec | Preprocessing records\n",
      "Total time: 24 min | Batch time: 138 sec | Storing records\n",
      "Merged pickle contains 2200989 records before merge\n",
      "Merged pickle contains 2451350 records after merge\n",
      "Total time: 24 min | Batch time: 156 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (11): year_week 201909 and 201913 (exclusive)\n",
      "Total time: 24 min | Batch time: 0 sec | Querying records\n",
      "Total time: 26 min | Batch time: 115 sec | Preprocessing records\n",
      "Total time: 27 min | Batch time: 166 sec | Storing records\n",
      "Merged pickle contains 2451350 records before merge\n",
      "Merged pickle contains 2731610 records after merge\n",
      "Total time: 27 min | Batch time: 188 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (12): year_week 201913 and 201917 (exclusive)\n",
      "Total time: 27 min | Batch time: 0 sec | Querying records\n",
      "Total time: 29 min | Batch time: 118 sec | Preprocessing records\n",
      "Total time: 30 min | Batch time: 173 sec | Storing records\n",
      "Merged pickle contains 2731610 records before merge\n",
      "Merged pickle contains 3010452 records after merge\n",
      "Total time: 31 min | Batch time: 199 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (13): year_week 201917 and 201921 (exclusive)\n",
      "Total time: 31 min | Batch time: 0 sec | Querying records\n",
      "Total time: 33 min | Batch time: 125 sec | Preprocessing records\n",
      "Total time: 34 min | Batch time: 178 sec | Storing records\n",
      "Merged pickle contains 3010452 records before merge\n",
      "Merged pickle contains 3298441 records after merge\n",
      "Total time: 34 min | Batch time: 209 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (14): year_week 201921 and 201925 (exclusive)\n",
      "Total time: 34 min | Batch time: 0 sec | Querying records\n",
      "Total time: 37 min | Batch time: 136 sec | Preprocessing records\n",
      "Total time: 38 min | Batch time: 195 sec | Storing records\n",
      "Merged pickle contains 3298441 records before merge\n",
      "Merged pickle contains 3586629 records after merge\n",
      "Total time: 38 min | Batch time: 227 sec | Records stored\n",
      "========================================\n",
      "Initiating batch (15): year_week 201925 and 201929 (exclusive)\n",
      "Total time: 38 min | Batch time: 0 sec | Querying records\n",
      "Total time: 39 min | Batch time: 30 sec | Preprocessing records\n",
      "Total time: 39 min | Batch time: 43 sec | Storing records\n",
      "Merged pickle contains 3586629 records before merge\n",
      "Merged pickle contains 3657490 records after merge\n",
      "Total time: 39 min | Batch time: 69 sec | Records stored\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "batch_num = 1\n",
    "\n",
    "#Get records for 2018\n",
    "min_date = 201823\n",
    "max_date = 201852 #201852\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201852:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        process_batch(start_time, batch_num, lower_date_range, upper_date_range)\n",
    "        lower_date_range=upper_date_range\n",
    "        batch_num += 1\n",
    "\n",
    "#Get records for 2019        \n",
    "min_date = 201901\n",
    "max_date = 201925 #201925\n",
    "lower_date_range = min_date\n",
    "upper_date_range = lower_date_range + 4\n",
    "while upper_date_range <=max_date:\n",
    "    if upper_date_range <=201925:\n",
    "        upper_date_range = lower_date_range + 4\n",
    "        process_batch(start_time, batch_num, lower_date_range, upper_date_range)\n",
    "        lower_date_range=upper_date_range\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_merged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('test_merged.pickle').sample(frac=0.25, replace=False).to_pickle('test_merged_sample.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
